# ğŸ§ª Sentiric LLM Local Service - Test ProsedÃ¼rleri

Bu dokÃ¼man, servisin baÅŸarÄ±yla kurulduÄŸunu ve beklendiÄŸi gibi Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrulamak iÃ§in gereken adÄ±mlarÄ± iÃ§erir.

## 1. Ã–n KoÅŸullar

- Docker ve Docker Compose'un sisteminizde kurulu olmasÄ±.
- `.env` dosyasÄ±nÄ±n projenin kÃ¶k dizininde bulunmasÄ± ve gerekli ortam deÄŸiÅŸkenlerini iÃ§ermesi.

## 2. Servisleri BaÅŸlatma

AÅŸaÄŸÄ±daki komut ile CPU tabanlÄ± servisleri baÅŸlatÄ±n. Bu komut, ilk Ã§alÄ±ÅŸtÄ±rmada model dÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemi nedeniyle uzun sÃ¼rebilir.

```bash
docker compose -f 'docker-compose.cpu.yml' up --build
```

## 3. DoÄŸrulama AdÄ±mlarÄ±

Servisler baÅŸarÄ±yla baÅŸladÄ±ktan sonra aÅŸaÄŸÄ±daki kontrolleri yapÄ±n:

### 3.1. Konteyner DurumlarÄ±

`docker ps` komutunu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, `llm-local-service-cpu` konteynerinin "Up" (Ã‡alÄ±ÅŸÄ±yor) ve "healthy" (SaÄŸlÄ±klÄ±) durumda olduÄŸunu gÃ¶rmelisiniz. `llm-model-converter` konteyneri ise iÅŸini bitirip durmuÅŸ olmalÄ±dÄ±r.

### 3.2. SaÄŸlÄ±k (Health) Endpoint'i

AÅŸaÄŸÄ±daki `curl` komutunu veya bir tarayÄ±cÄ±yÄ± kullanarak HTTP saÄŸlÄ±k kontrolÃ¼ endpoint'ini sorgulayÄ±n:

```bash
curl http://localhost:16060/health
```

**Beklenen BaÅŸarÄ±lÄ± YanÄ±t:**
```json
{
  "status": "healthy",
  "model_ready": true,
  "model_name": "microsoft/Phi-3-mini-4k-instruct",
  "device": "cpu"
}
```
EÄŸer `model_ready` deÄŸeri `false` ise, modelin yÃ¼klenmesi henÃ¼z tamamlanmamÄ±ÅŸ olabilir. BirkaÃ§ saniye bekleyip tekrar deneyin.

### 3.3. Metrik (Metrics) Endpoint'i

Prometheus metriklerinin doÄŸru bir ÅŸekilde yayÄ±nlandÄ±ÄŸÄ±nÄ± kontrol edin:

```bash
curl http://localhost:16062/metrics
```

**Beklenen BaÅŸarÄ±lÄ± YanÄ±t:**
HTTP istekleri, Python garbage collector vb. ile ilgili bir dizi Prometheus metrik Ã§Ä±ktÄ±sÄ± gÃ¶rmelisiniz.

### 3.4. gRPC Test Ä°stemcisi

Servisin ana iÅŸlevselliÄŸi olan token streaming'i test etmek iÃ§in projenin yerleÅŸik gRPC test istemcisini Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python grpc_test_client.py "TÃ¼rkiye'nin baÅŸkenti neresidir?"
```

**Beklenen BaÅŸarÄ±lÄ± YanÄ±t:**
Terminalde, modelin bu soruya token token Ã¼rettiÄŸi bir yanÄ±tÄ±n akÄ±cÄ± bir ÅŸekilde yazÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶rmelisiniz. Ã–rnek:

```
ğŸ”Œ Sunucuya baÄŸlanÄ±lÄ±yor: localhost:16061
ğŸ’¬ GÃ¶nderilen Prompt: 'TÃ¼rkiye'nin baÅŸkenti neresidir?'
--- AI YanÄ±tÄ± ---
TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r.
-------------------
âœ… AkÄ±ÅŸ baÅŸarÄ±yla tamamlandÄ±.
```

Bu adÄ±mlarÄ±n tÃ¼mÃ¼ baÅŸarÄ±yla tamamlandÄ±ysa, servis doÄŸru bir ÅŸekilde kurulmuÅŸ ve Ã§alÄ±ÅŸÄ±yor demektir.


---

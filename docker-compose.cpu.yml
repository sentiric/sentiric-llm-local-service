services:
  llm-local-service-cpu:
    build:
      context: .
      # CPU için olan Dockerfile'ı kullanıyoruz
      dockerfile: Dockerfile
    container_name: llm-local-service-cpu
    environment:
      # Modeli Phi-4 olarak ayarlıyoruz
      - LLM_LOCAL_SERVICE_MODEL_NAME=microsoft/Phi-3-mini-4k-instruct
      # Motora CPU'da çalışmasını söylüyoruz
      - LLM_LOCAL_SERVICE_DEVICE=cpu
      - LOG_LEVEL=info
    volumes: 
      # Modellerin indirileceği ve kalıcı olacağı yerler
      - llm_model_cache:/app/model-cache
      - llm_hf_cache:/root/.cache/huggingface
    ports: 
      - "16060:16060" # HTTP Health
      - "16061:16061" # gRPC
      - "16062:16062" # Prometheus Metrics
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:16060/health"]
      interval: 30s
      timeout: 20s
      retries: 5
      # Modelin indirilip CPU'da yüklenmesi çok uzun sürebilir, bu yüzden süreyi cömert tutuyoruz
      start_period: 25m

volumes:
  llm_model_cache:
  llm_hf_cache: